# Try 2010 only ----
https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/
library("xml2")
library("rvest")
library("dplyr")
library("terra")
# Try 2010 only ----
https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/
# Try 2010 only ----
https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
library("xml2")
library("rvest")
library("dplyr")
library("terra")
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("Canada/NDVI/NOAA_Files/",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("Canada/NDVI/NOAA_Files/",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#test the files to see if they work
file1 <- "Canada/NDVI/NOAA_Files/AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810627_c20170610050500.nc"
NDVI <- terra::rast(file2)
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated"
NDVI <- terra::rast(file2)
# set working directory
setwd("C:/Users/justi/Documents/GitHub/HWI_parks")
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated"
NDVI <- terra::rast(file2)
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parks/data/AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated"
NDVI <- terra::rast(file2)
getwd
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parks/data/AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated"
NDVI <- terra::rast(file2)
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
NDVI <- terra::rast(file2)
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
NDVI <- terra::rast(file2)
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
NDVI <- terra::rast(file2)
Sys.time(plot(NDVI[[1]]))
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
unique(file2)
file2new <- na.omit(file2)
NDVI <- terra::rast(file2new)
NDVI <- terra::rast(file1)
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
NDVI <- terra::rast(file1)
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
file2new <- na.omit(file2)
NDVI <- terra::rast(file1)
NDVI <- terra::rast(file2new)
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access"
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract all links for each year
url_path <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/"
pg <- read_html(url_path)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url_path, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
file2new <- na.omit(file2)
NDVI <- terra::rast(file2new)
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
NDVI <- terra::rast(file2)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
NDVI <- terra::rast(file2)
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc	"
NDVI <- terra::rast(file2)
file2 <- "AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc	"
NDVI <- terra::rast(file2)
file2 <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/AVHRR-Land_v005_AVH13C1_NOAA-19_20101230_c20170406210004.nc"
NDVI <- terra::rast(file2new)
View(pag)
View(pag)
View(pg)
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parksNA/AVHRR-Land_v005_AVH13C1_NOAA-19_20101231_c20170406211535.nc	"
NDVI <- terra::rast(file2new)
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parksNA"
NDVI <- terra::rast(file2new)
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parksNA/AVHRR-Land_v005_AVH13C1_NOAA-19_20101231_c20170406211535.nc"
NDVI <- terra::rast(file2)
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parksNA"
NDVI <- terra::rast(file2)
#extract links for each file in each year
for(i in 6:length(LINKS)){
url <- LINKS[i]
pag <- read_html(url)
ndvi_links <- paste(LINKS[i], html_attr(html_nodes(pag, "a"), "href"),  sep = "")
filenames <- html_attr(html_nodes(pag, "a"), "href")
for(j in 6:length(ndvi_links)){
url_path <- ndvi_links[j]
path <- paste("C:/Users/justi/Documents/GitHub/HWI_parks/",filenames[j], sep="")
try(download.file(url_path, destfile = path))
Sys.sleep(5)
}
Sys.sleep(5)
}
#test the files to see if they work
file1 <- "C:/Users/justi/Documents/GitHub/HWI_parksAVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc"
file2 <- "C:/Users/justi/Documents/GitHub/HWI_parksNA"
NDVI <- terra::rast(file1)
#test the files to see if they work
file1 <- "AVHRR-Land_v005_AVH13C1_NOAA-07_19810624_c20170610041337.nc_Annotated.rds"
NDVI <- terra::rast(file1)
