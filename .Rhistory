#4
animals_involved <- read.csv("pca-human-wildlife-coexistence-animals-involved-detailed-records-2010-2021.csv")
#5
responses <- read.csv("pca-human-wildlife-coexistence-responses-detailed-records-2010-2021.csv")
#6
activites <- read.csv("pca-human-wildlife-coexistence-activities-detailed-records-2010-2021.csv")
#7
incidents_by_incidents_type <- read.csv("pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#8
staff_time_incident <- read.csv("pca-hours-of-staff-time-by-incident-type-2010-2021.csv")
#9
number_incidents_species <- read.csv("pca-number-of-incidents-by-species-2010-2021.csv")
#10
staff_time_species <- read.csv("pca-hours-of-staff-time-by-species-2010-2021.csv")
#11
animals_killed_human_cause <- read.csv("pca-number-of-animals-killed-by-human-causes-2010-2021.csv")
#12
aggressive_encounters_species <- read.csv("pca-number-of-aggressive-encounters-by-species-2010-2021.csv")
#13
animals_invlved_unnatural_attractants <- read.csv("pca-number-of-animals-involved-with-unnatural-attractants-2010-2021.csv")
#14
animals_killed_collisions <-read.csv("pca-number-of-animals-killed-by-collisions-with-vehicles-and-trains-2010-2021.csv")
#15
incidents_by_response <- read.csv("pca-number-of-incidents-by-response-type-2010-2021.csv")
#16
incidents_by_site <- read.csv("pca-number-of-incidents-by-site-2010-2021.csv")
#17
staff_time_site <- read.csv("pca-hours-of-staff-time-by-site-2010-2021.csv")
#18
animals_killed_collisions_site <- read.csv("pca-number-of-animals-killed-by-collisions-with-vehicles-and-trains-by-site-2010-2021.csv")
#19
aggressive_encounters_species_site <- read.csv("pca-number-of-aggressive-encounters-by-species-and-by-site-2010-2021.csv")
#Choosing 4 (animals_involved), 9(number_incidents_species), and 19(aggressive_encounters_species_site).
skim_without_charts("animals-involved")
#filter out all the human wildlife interactions
HWI <- animals_involved %>%
filter(Incident.Type %in% c("Human Wildlife Interaction"))
unique(HWI$Protected.Heritage.Area)
# Loading packages ----
options(timeout = max(1000, getOption("timeout")))
library(lattice) # for making graphs
library(faraway) # for calculationg VIF
library(knitr) # for knitting
library(ggplot2) # for scatter plot
library(dplyr) # for pipes
library(formatR) # for formatting for knitting to pdf
library(skimr) # for skimming data
library(tidyverse) #summing
library("lubridate") #convert whole columns to dates
# Setting root directory ----
knitr::opts_knit$set(root.dir = 'C:/Users/gracelou/OneDrive - UBC/Documents/BIOL440')
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#4
animals_involved <- read.csv("pca-human-wildlife-coexistence-animals-involved-detailed-records-2010-2021.csv")
#5
responses <- read.csv("pca-human-wildlife-coexistence-responses-detailed-records-2010-2021.csv")
# Loading packages ----
options(timeout = max(1000, getOption("timeout")))
library(lattice) # for making graphs
library(faraway) # for calculationg VIF
library(knitr) # for knitting
library(ggplot2) # for scatter plot
library(dplyr) # for pipes
library(formatR) # for formatting for knitting to pdf
library(skimr) # for skimming data
library(tidyverse) #summing
library("lubridate") #convert whole columns to dates
# Setting root directory ----
knitr::opts_knit$set(root.dir = 'C:/Users/gracelou/OneDrive - UBC/Documents/BIOL440')
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
# Loading packages ----
options(timeout = max(1000, getOption("timeout")))
library(lattice) # for making graphs
library(faraway) # for calculationg VIF
library(knitr) # for knitting
library(ggplot2) # for scatter plot
library(dplyr) # for pipes
library(formatR) # for formatting for knitting to pdf
library(skimr) # for skimming data
library(tidyverse) #summing
library(lubridate) #convert whole columns to dates
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("data/pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#7
incidents_by_incidents_type <- read.csv("data/pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
#8
staff_time_incident <- read.csv("data/pca-hours-of-staff-time-by-incident-type-2010-2021.csv")
#9
number_incidents_species <- read.csv("data/pca-number-of-incidents-by-species-2010-2021.csv")
#10
staff_time_species <- read.csv("data/pca-hours-of-staff-time-by-species-2010-2021.csv")
#11
animals_killed_human_cause <- read.csv("data/pca-number-of-animals-killed-by-human-causes-2010-2021.csv")
#12
aggressive_encounters_species <- read.csv("data/pca-number-of-aggressive-encounters-by-species-2010-2021.csv")
#13
animals_invlved_unnatural_attractants <- read.csv("data/pca-number-of-animals-involved-with-unnatural-attractants-2010-2021.csv")
#14
animals_killed_collisions <-read.csv("data/pca-number-of-animals-killed-by-collisions-with-vehicles-and-trains-2010-2021.csv")
#15
incidents_by_response <- read.csv("data/pca-number-of-incidents-by-response-type-2010-2021.csv")
#16
incidents_by_site <- read.csv("data/pca-number-of-incidents-by-site-2010-2021.csv")
#17
staff_time_site <- read.csv("data/pca-hours-of-staff-time-by-site-2010-2021.csv")
#18
animals_killed_collisions_site <- read.csv("data/pca-number-of-animals-killed-by-collisions-with-vehicles-and-trains-by-site-2010-2021.csv")
#19
aggressive_encounters_species_site <- read.csv("data/pca-number-of-aggressive-encounters-by-species-and-by-site-2010-2021.csv")
#Skim animals involved data ----
skim_without_charts("animals-involved")
#filter out all the human wildlife interactions ----
HWI <- animals_involved %>%
filter(Incident.Type %in% c("Human Wildlife Interaction"))
#visualize the number of individuals per species ----
ggplot(data = HWI, aes(x = Species.Common.Name, y = Sum.of.Number.of.Animals)) +
geom_bar(stat = "identity", position = position_dodge()) +
ylab("Frequency") +
xlab("Species") +
theme_bw()
#Cleaning the first nations heritage site in HWI data ----
HWI$Protected.Heritage.Area[HWI$Protected.Heritage.Area == "Saoy\xfa-?ehdacho National Historic Site of Canada"]<- "Grizzly Bear Mountain and Scented Grass Hills"
#visualize the number of individuals per region ----
ggplot(data = HWI, aes(x = Protected.Heritage.Area, y = Sum.of.Number.of.Animals)) +
geom_bar(stat = "identity", position = position_dodge()) +
ylab("Frequency") +
xlab("Region") +
theme_bw()
# count the number of regions and the number of species in HWI ----
regioncount <- HWI %>%
count(HWI$Protected.Heritage.Area)
speciescount <- HWI %>%
count(HWI$Species.Common.Name)
# count the number of regions with aggressive encounters ----
aggressive_regioncount <- aggressive_encounters_species_site %>%
count(aggressive_encounters_species_site$Protected.Heritage.Area)
# count the number of each species engaged in aggressive encounters ----
aggressive_speciescount <- aggressive_encounters_species_site %>%
count(aggressive_encounters_species_site$Species.Common.Name)
# Convert dates in HWI from characters to date ----
HWI$Incident.Date <- ymd(HWI$Incident.Date)
# Add a column "Incident Year" to HWI ----
HWI$Incident.Year <- as.numeric(format(HWI$Incident.Date, "%Y"))
# Count incident by year in HWI ----
incidentcountbyyear <- HWI %>%
count(HWI$Incident.Year)
#filter out all the human wildlife interactions ----
HWI <- animals_involved %>%
filter(Incident.Type %in% c("Human Wildlife Interaction"))
#importing all the datasets to see which ones are useful ----
#3
coexistence_incidents_record <- read.csv("data/pca-human-wildlife-coexistence-incidents-detailed-records-2010-2021.csv")
Parkpolygon<-readOGR("data/Reserves_of_Canada_Legislative_Boundaries.shp")
library(rgdal)
Parkpolygon<-readOGR("data/Reserves_of_Canada_Legislative_Boundaries.shp")
shape <- read_sf(dsn = "data/Reserves_of_Canada_Legislative_Boundaries.shp", layer = "SHAPEFILE")
library(lattice) # for making graphs
library(knitr) # for knitting
library(ggplot2) # for scatter plot
library(dplyr) # for pipes
library(skimr) # for skimming data
library(tidyverse) #summing
library(lubridate) #convert whole columns to dates
library(zoo) #dates as year month
library(canadianmaps) #import annotated map of Canada
library(sf) # spatial data
library(sp) #Spatial Points function
library(rstudioapi) #for creating colour palette
library(grDevices) #for creating colour palette
library(fBasics) #for creating colour palette
library(mgcv) #gam
library(terra) #shape file
# importing shape files of Canadian national parks
terra::vect("data/Reserves_of_Canada_Legislative_Boundaries.shp")
vect("data/Reserves_of_Canada_Legislative_Boundaries.shp")
shape <- read_sf(dsn = "data/Reserves_of_Canada_Legislative_Boundaries.shp", layer = "SHAPEFILE")
st_read("data/Reserves_of_Canada_Legislative_Boundaries.shp")
library(rgdal)
# importing shape files of Canadian national parks
terra::vect("data/Reserves_of_Canada_Legislative_Boundaries.shp")
# importing shape files of Canadian national parks
terra::vect("data/Parkpolygon")
parkpolygon <- system.file("data/Reserves_of_Canada_Legislative_Boundaries", package="terra")
terra::vect(f)
plot(parkpolygon)
ggplot() +
geom_polygon(data = parkpolygon, aes( x = long, y = lat, group = group), fill="#69b3a2", color="white") +
theme_void()
parkpolygon <- system.file("data/Reserves_of_Canada_Legislative_Boundaries", package="terra")
terra::vect(f)
# importing shape files of Canadian national parks
terra::vect("data/Reserves_of_Canada_Legislative_Boundaries.shp")
st_read("data/Reserves_of_Canada_Legislative_Boundaries.shp")
shape <- read_sf(dsn = "data/Reserves_of_Canada_Legislative_Boundaries.shp", layer = "SHAPEFILE")
# importing shape files of Canadian national parks
terra::vect("data/Reserves_of_Canada_Legislative_Boundaries.shp")
Parkpolygon<-readOGR("data/Reserves_of_Canada_Legislative_Boundaries.shp")
library("xml2")
library("rvest")
library("dplyr")
library("terra")
#extract all links for 2010 [done] ----
url <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/"
pg <- read_html(url)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
for(i in 1:length(linkys)){
link <- paste(url, linkys[i], sep = "")
LINKS[i] <- link
}
LINKS <- do.call(rbind, LINKS)
# redownload all the links and now it should work
for(j in 6:length(linkys)){
url_path <- paste(url, linkys[j], sep = "")
path <- paste("C:/Users/grace/Documents/GitHub/HWI_parks/2010ndvi/",linkys[j], sep="")
try(download.file(url_path, destfile = path, mode = "wb")) #add mode = wb and now it works --> the probably won't have to run corrupt file unless things don't work
Sys.sleep(5)
}
# Loading packages ----
options(timeout = max(1000, getOption("timeout")))
library(lattice) # for making graphs
library(knitr) # for knitting
library(ggplot2) # for scatter plot
library(dplyr) # for pipes
library(skimr) # for skimming data
library(tidyverse) #summing
library(lubridate) #convert whole columns to dates
library(zoo) #dates as year month
library(canadianmaps) #import annotated map of Canada
library(sf) # spatial data
library(sp) #Spatial Points function
library(rstudioapi) #for creating colour palette
library(grDevices) #for creating colour palette
library(fBasics) #for creating colour palette
library(mgcv) #gam
library(terra) #shape file
# set working directory
setwd("C:/Users/grace/Documents/GitHub/HWI_parks")
ABpolygon <- st_read("data/CLAB_AB_2023-09-08/CLAB_AB_2023-09-08.shp")
BCpolygon <- st_read("data/CLAB_BC_2023-09-08/CLAB_BC_2023-09-08.shp")
MBpolygon <- st_read("data/CLAB_MB_2023-09-08/CLAB_MB_2023-09-08.shp")
NBpolygon <- st_read("data/CLAB_NB_2023-09-08/CLAB_NB_2023-09-08.shp")
NLpolygon <- st_read("data/CLAB_NL_2023-09-08/CLAB_NL_2023-09-08.shp")
NSpolygon <- st_read("data/CLAB_NS_2023-09-08/CLAB_NS_2023-09-08.shp")
NTpolygon <- st_read("data/CLAB_NT_2023-09-08/CLAB_NT_2023-09-08.shp")
NUpolygon <- st_read("data/CLAB_NU_2023-09-08/CLAB_NU_2023-09-08.shp")
ONpolygon <- st_read("data/CLAB_ON_2023-09-08/CLAB_ON_2023-09-08.shp")
PEpolygon <- st_read("data/CLAB_PE_2023-09-08/CLAB_PE_2023-09-08.shp")
QCpolygon <- st_read("data/CLAB_QC_2023-09-08/CLAB_QC_2023-09-08.shp")
SKpolygon <- st_read("data/CLAB_SK_2023-09-08/CLAB_SK_2023-09-08.shp")
YTpolygon <- st_read("data/CLAB_YT_2023-09-08/CLAB_YT_2023-09-08.shp")
waterton_lakes <- ABpolygon[ABpolygon$CLAB_ID == "WATE", ]
plot(waterton_lakes)
elk_island <- ABpolygon[ABpolygon$CLAB_ID == "ELKI", ]
plot(elk_island)
jasper <- ABpolygon[ABpolygon$CLAB_ID == "JASP", ]
plot(jasper)
wood_buffalo <- ABpolygon[ABpolygon$CLAB_ID == "WOOD", ]
plot(wood_buffalo)
banff <- ABpolygon[ABpolygon$CLAB_ID == "BANF", ]
plot(banff)
yoho <- BCpolygon[BCpolygon$CLAB_ID == "YOHO", ]
plot(yoho)
kootenay <- BCpolygon[BCpolygon$CLAB_ID == "KOOT", ]
plot(kootenay)
mount_revelstoke <- BCpolygon[BCpolygon$CLAB_ID == "REVE", ]
plot(mount_revelstoke)
pacific_rim <- BCpolygon[BCpolygon$CLAB_ID == "PRIM", ]
plot(pacific_rim)
glacier <- BCpolygon[BCpolygon$CLAB_ID == "GLAC", ]
plot(glacier)
wapusk <- MBpolygon[MBpolygon$CLAB_ID == "WAPU", ]
plot(wapusk)
fundy <- NBpolygon[NBpolygon$CLAB_ID == "FUND", ]
plot(fundy)
kouchibouguac <- NBpolygon[NBpolygon$CLAB_ID == "KOUC", ]
plot(kouchibouguac)
terra_nova <- NLpolygon[NLpolygon$CLAB_ID == "NOVA", ]
plot(terra_nova)
kejimkujik <- NSpolygon[NSpolygon$CLAB_ID == "KEJI", ]
plot(kejimkujik)
aulavik <- NTpolygon[NTpolygon$CLAB_ID == "AULA", ]
plot(aulavik)
nahanni <- NTpolygon[NTpolygon$CLAB_ID == "NAHA", ]
plot(nahanni)
fathom_five <- ONpolygon[ONpolygon$CLAB_ID == "FIVE", ]
plot(fathom_five)
point_pelee <- ONpolygon[ONpolygon$CLAB_ID == "PELE", ]
plot(point_pelee)
georgian_bay_islands <- ONpolygon[ONpolygon$CLAB_ID == "GBIS", ]
plot(georgian_bay_islands)
thousand_islands <- ONpolygon[ONpolygon$CLAB_ID == "THIS", ]
plot(thousand_islands)
prince_edward_island <- PEpolygon[PEpolygon$CLAB_ID == "PEIS", ]
plot(prince_edward_island)
forillon <- QCpolygon[QCpolygon$CLAB_ID == "FORI", ]
plot(forillon)
prince_albert <- SKpolygon[SKpolygon$CLAB_ID == "PALB", ]
plot(prince_albert)
ivvavik <- YTpolygon[YTpolygon$CLAB_ID == "IVVA", ]
plot(ivvavik)
# merge all park polygons into one shape file ----
parks_polygon <- dplyr::bind_rows(list(waterton_lakes,elk_island,jasper, wood_buffalo, banff, yoho, kootenay, mount_revelstoke, pacific_rim, glacier, wapusk, fundy, kouchibouguac, terra_nova, kejimkujik, aulavik, nahanni, fathom_five, point_pelee, georgian_bay_islands, thousand_islands, prince_edward_island, forillon, prince_albert, ivvavik))
plot(parks_polygon)
plot(parks_polygon$geometry)
parks_geometry <- parks_polygon$geometry
plot(parks_geometry)
# rekha's code to rasterise the downloaded ndvi ----
# 2010ndvi----
# make a dataframe
jan_2010ndvi <- unique(list.files(path = 'C:/Users/grace/Documents/GitHub/HWI_parks/2010ndvi/2010_jan/', # file for jan
pattern = ".nc", full.names = T))
jan2010ndvi <- list()
for(i in 1:length(jan_2010ndvi)){
r <- terra::rast(jan_2010ndvi[i])
c <- crop(r, parks_geometry) # crop to my polygon
jan2010ndvi[[i]] <- c
} #now they're all added onto the list
jan2010 <- rast(jan2010ndvi) # rasterise the list
trial <- stack(jan2010) # stack the rasters, why does each day have 3 layers?
mean <- calc(jan2010, mean) # calculate mean of the month --> error
library("xml2")
library("rvest")
library("dplyr")
library("terra")
library("raster")
#extract all links for 2010 [done] ----
url <- "https://www.ncei.noaa.gov/data/land-normalized-difference-vegetation-index/access/2010/"
pg <- read_html(url)
linkys <- html_attr(html_nodes(pg, "a"), "href")
LINKS <- list()
# rekha's code to rasterise the downloaded ndvi ----
# 2010ndvi----
# make a dataframe
jan_2010ndvi <- unique(list.files(path = 'C:/Users/grace/Documents/GitHub/HWI_parks/2010ndvi/2010_jan/', # file for jan
pattern = ".nc", full.names = T))
jan2010ndvi <- list()
for(i in 1:length(jan_2010ndvi)){
r <- terra::rast(jan_2010ndvi[i])
c <- crop(r, parks_geometry) # crop to my polygon
jan2010ndvi[[i]] <- c
} #now they're all added onto the list
jan2010 <- rast(jan2010ndvi) # rasterise the list
trial <- stack(jan2010) # stack the rasters, why does each day have 3 layers?
mean <- calc(jan2010, mean) # calculate mean of the month --> error
plot(mean) # plot mean of jan raster --> weird
plot(trial) # why does each day has 3 layers?
jan2010i<- rast(jan2010$NDVI)
jan2010ii <- dropLayer(jan2010, TIMEOFDAY + QA)
trialii <- dropLayer(trial, TIMEOFDAY + QA)
trialii <- dropLayer(trial, TIMEOFDAY.1 + QA.1)
class(trial)
layer_sf(trial)
unique(trial$TIMEOFDAY.1)
trialii <- dropLayer(trial, c(2,3))
plot(trialii)
trialii <- dropLayer(trial, c(2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24,26,27,29,30,32,33,35,36,38,39,41,42,44,45,47,48,50,51,53,54,56,57,59,60,62,63,65,66,68,69,71,72,74,75,77,78,80,81,83,84,86,87,89,90,92,93))
plot(trialii)
jan2010ndvi_only <- dropLayer(trial, c(2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24,26,27,29,30,32,33,35,36,38,39,41,42,44,45,47,48,50,51,53,54,56,57,59,60,62,63,65,66,68,69,71,72,74,75,77,78,80,81,83,84,86,87,89,90,92,93))
plot(jan2010ndvi_only)
mean <- calc(jan2010ndvi_only, mean) # calculate mean of the month --> error
plot(mean) # plot mean of jan raster --> weird
View(parks_geometry)
for(i in 1:length(jan_2010ndvi)){
r <- terra::rast(jan_2010ndvi[i])
#c <- crop(r, parks_geometry) # crop to my polygon
jan2010ndvi[[i]] <- c
} #now they're all added onto the list
jan2010 <- rast(jan2010ndvi) # rasterise the list
trial <- stack(jan2010) # stack the rasters, each day has 3 layers
# drop the timeodday and qa layers
jan2010ndvi_only <- dropLayer(trial, c(2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24,26,27,29,30,32,33,35,36,38,39,41,42,44,45,47,48,50,51,53,54,56,57,59,60,62,63,65,66,68,69,71,72,74,75,77,78,80,81,83,84,86,87,89,90,92,93))
plot(jan2010ndvi_only)
mean <- calc(jan2010ndvi_only, mean) # calculate mean of the month
plot(mean) # plot mean of jan raster
trialcrop <- crop(trial, parks_geometry)
trialcrop <- mask(trial, parks_geometry)
trialcrop <- mask(trial, parks_geometry)
trialcrop <- crop(trial, parks_geometry)
cr <- crop(trial, extent(parks_geometry), snap="out")
# Plot full raster and polygon
plot(trial)
# drop the timeodday and qa layers
jan2010ndvi_only <- dropLayer(trial, c(2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24,26,27,29,30,32,33,35,36,38,39,41,42,44,45,47,48,50,51,53,54,56,57,59,60,62,63,65,66,68,69,71,72,74,75,77,78,80,81,83,84,86,87,89,90,92,93))
# Plot full raster and polygon
plot(jan2010ndvi_only)
plot(parks_geometry,add=T)
# Plot full raster and polygon
plot(jan2010ndvi_only$NDVI.1)
plot(parks_geometry,add=T)
#test the files to see if they can plot ndvi [done]
file1 <- "2010ndvi/2010_jan/AVHRR-Land_v005_AVH13C1_NOAA-19_20100101_c20170406091314.nc"
file2 <- "2010ndvi/2010_dec/AVHRR-Land_v005_AVH13C1_NOAA-19_20101231_c20170406211535.nc"
NDVI <- terra::rast(file1)
plot(NDVI$NDVI) # IT'S PLOTTING :DDDD
# importing shape files of Canadian national parks (visualisation only)----
CAshape <- vect("data/CLAB_CA_2023-09-08/CLAB_CA_2023-09-08.shp")
plot(CAshape)
# Crop using extent, rasterize polygon and finally, create poly-raster
#          **** This is the code that you are after ****
cr <- crop(jan2010ndvi_only, extent(parks_geometry), snap="out")
extent(jan2010ndvi_only)
extent(parks_geometry)
# Crop using extent, rasterize polygon and finally, create poly-raster
#          **** This is the code that you are after ****
cr <- extract(jan2010ndvi_only, extent(parks_geometry), snap="out")
newExtent <- extent(bbox(parks_geometry))
library(rgdal)
install(rgdal)
install.packages(rgdal)
install.packages("rgdal")
install.packages("Rtools")
install.packages("C:/Users/grace/Downloads/rgdal_1.6-7.tar.gz", repos = NULL, type = "source")
library(rgdal)
newExtent <- extent(bbox(parks_geometry))
trialcrop <- crop(trial, parks_geometry)
load("~/Bamfield RMCW/Lab 3/.RData")
source(file = "gdal.R")
source(file = "gdal.R")
newExtent <- extent(bbox(parks_geometry))
parks_geometry
parks_geometry(st_cast(x, "MULTIPOLYGON"))
mapview(st_cast(parks_geometry, "MULTIPOLYGON"))
installed.packages("mapview")
library(mapview)
install.packages("mapview")
mapview(st_cast(parks_geometry, "MULTIPOLYGON"))
library(mapview)
mapview(st_cast(parks_geometry, "MULTIPOLYGON"))
parks_geometry
plot(parks_geometry, add = T)
mapview(sf::st_cast(parks_geometry, "MULTIPOLYGON"))
mapview(sf::st_cast(parks_geometry, "POLYGON"))
warnings()
install.packages("gdalraster")
mapview(sf::st_cast(parks_geometry, "MULTIPOLYGON"))
# rekha's code to rasterise the downloaded ndvi ----
# 2010ndvi----
# make a dataframe
jan_2010ndvi <- unique(list.files(path = 'C:/Users/grace/Documents/GitHub/HWI_parks/2010ndvi/2010_jan/', # file for jan
pattern = ".nc", full.names = T))
jan2010ndvi <- list()
for(i in 1:length(jan_2010ndvi)){
r <- terra::rast(jan_2010ndvi[i])
#c <- crop(r, parks_geometry) # crop to my polygon --> crop not working?
jan2010ndvi[[i]] <- c
} #now they're all added onto the list
jan2010 <- rast(jan2010ndvi) # rasterise the list
trial <- stack(jan2010) # stack the rasters, each day has 3 layers
# drop the timeodday and qa layers
jan2010ndvi_only <- dropLayer(trial, c(2,3,5,6,8,9,11,12,14,15,17,18,20,21,23,24,26,27,29,30,32,33,35,36,38,39,41,42,44,45,47,48,50,51,53,54,56,57,59,60,62,63,65,66,68,69,71,72,74,75,77,78,80,81,83,84,86,87,89,90,92,93))
plot(jan2010ndvi_only)
# calculate mean of the month
mean <- calc(jan2010ndvi_only, mean)
# calculate mean of the month
mean <- calc(jan2010ndvi_only, mean)
plot(mean) # plot mean of jan raster
extent(jan2010ndvi_only)
extent(banff)
extent(parks_polygon)
extent(parks_geometry)
trialcrop <- crop(trial, parks_polygon)
trialcrop <- crop(jan2010ndvi_only, parks_polygon)
plot(trialcrop)
# rekha's code to rasterise the downloaded ndvi ----
# 2010ndvi----
# make a dataframe
jan_2010ndvi <- unique(list.files(path = 'C:/Users/grace/Documents/GitHub/HWI_parks/2010ndvi/2010_jan/', # file for jan
pattern = ".nc", full.names = T))
jan2010ndvi <- list()
for(i in 1:length(jan_2010ndvi)){
r <- terra::rast(jan_2010ndvi[i])
c <- crop(r, parks_geometry) # crop to my polygon --> crop not working?
jan2010ndvi[[i]] <- c
} #now they're all added onto the list
jan2010 <- rast(jan2010ndvi) # rasterise the list
trial <- stack(jan2010) # stack the rasters, each day has 3 layers
plot(trial$NDVI.1[1])
ext(parks_polygon)
head(parks_polygon)
plot(parks_geometry)
i <- 1
r <- terra::rast(jan_2010ndvi[i])
c <- crop(r, parks_geometry) # crop to my polygon --> crop not working?
plot(r$NDVI)
plot(c$NDVI)
c <- crop(r, parks_polygon) # crop to my polygon --> crop not working?
plot(c$NDVI)
plot(parks_polygon, add = T)
parks_polygon
View(parks_polygon)
parks_polygon
plot(mean) # plot mean of jan raster
plot(parks_polygon, add = T)
save(foo,jan2010ndvi="data.Rda")
save(foo,file="jan2010ndvi.Rda")
saveRDS(file = "jan2010ndvi.Rda")
mean
